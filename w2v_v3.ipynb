{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/beidan/AL-word2vec', '/home/beidan/miniconda3/lib/python38.zip', '/home/beidan/miniconda3/lib/python3.8', '/home/beidan/miniconda3/lib/python3.8/lib-dynload', '', '/home/beidan/.local/lib/python3.8/site-packages', '/home/beidan/miniconda3/lib/python3.8/site-packages', '/home/beidan/miniconda3/lib/python3.8/site-packages/IPython/extensions', '/home/beidan/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save a smaller patentdb input\n",
    "# aaa = []\n",
    "# with open('/home/beidan/AutoPhrase/data/EN/patentdb.txt') as f:\n",
    "#     i=0\n",
    "#     for line in f:\n",
    "#         if(len(line)<5):continue\n",
    "#         line=line.replace('\\n','')\n",
    "#         if i>5000: continue\n",
    "#         aaa.append(line)\n",
    "#         i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/beidan/AutoPhrase/data/EN/patentdb-medium.txt', 'w') as f:\n",
    "#     f.writelines(\"%s\\n\" % line for line in aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/beidan/AutoPhrase/models/DBLP/segmentation.txt') as content:\n",
    "#     i=0\n",
    "#     with open('patentdb_seg_1K','w') as f:\n",
    "#         for line in content:\n",
    "#             if(i>2000):continue\n",
    "#             if(len(line)<5):continue\n",
    "#             f.write(line)\n",
    "#             i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Formatted Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tag_strings(tag_str):\n",
    "    pattern = '<phrase>(.*?)</phrase>'\n",
    "    matches = re.findall(pattern, tag_str, re.S)  # This returns a list of all matches\n",
    "    for match in matches:\n",
    "#         str_to_replace = match.group(1)  # 1 index is the string in between the tags\n",
    "        tag_str = tag_str.replace(match, match.strip().replace(' ', '_'))\n",
    "    return tag_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams_embeded(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return ['_'.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "def convert_ngram(line, n):\n",
    "    return ' '.join(get_ngrams_embeded(line, n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_quality_phrases(lines):\n",
    "    quality_phrases=set()\n",
    "    for line in lines:\n",
    "        soup = BeautifulSoup(line, 'html.parser')\n",
    "        phrases = soup.findAll('phrase')\n",
    "        for p in phrases:\n",
    "            p = p.text\n",
    "            quality_phrases.add(p)\n",
    "    return quality_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "original_corpus=[]\n",
    "def get_raw_corpus():\n",
    "      with open('input/segmentation.txt') as content:\n",
    "        for line in content:\n",
    "            if(len(line)<5):continue\n",
    "            line=line.replace('\\n','')\n",
    "            original_corpus.append(line)  \n",
    "            \n",
    "get_raw_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = round(len(original_corpus) * 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_phrases_train_set = get_quality_phrases(original_corpus[:train_len])\n",
    "quality_phrases_test_set = get_quality_phrases(original_corpus[train_len+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_phrases_train = [p.replace(' ', '_') for p in (quality_phrases_train_set)]\n",
    "quality_phrases_train = [p.lower() for p in (quality_phrases_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_phrases_test = [p.replace(' ', '_') for p in (quality_phrases_test_set)]\n",
    "quality_phrases_test = [p.lower() for p in (quality_phrases_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['identity_registration_database',\n",
       " 'trail',\n",
       " 'musical_instrument',\n",
       " 'personalization',\n",
       " 'drive_source']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_phrases_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "lines = []\n",
    "\n",
    "def get_corpus():\n",
    "    with open('input/segmentation.txt') as content:\n",
    "        for line in content:\n",
    "            if(len(line)<5):continue\n",
    "#             line=process_tag_strings(str(line))\n",
    "            #senario 2: convert to all bi-grams\n",
    "            line=cleanhtml(line)\n",
    "            line = convert_ngram(line, 2)\n",
    "            lines.append(line)\n",
    "            line=line.replace('\\n','')\n",
    "            articles.append(line)\n",
    "            \n",
    "get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_lines=[]\n",
    "\n",
    "def get_corpus_unprocessed():\n",
    "    with open('input/segmentation.txt') as content:\n",
    "        for line in content:\n",
    "            if(len(line)<5):continue\n",
    "#             line=process_tag_strings(str(line))\n",
    "            lines.append(line)\n",
    "            line=cleanhtml(line)\n",
    "            line=line.replace('\\n','')\n",
    "            unprocessed_lines.append(line)\n",
    "            \n",
    "get_corpus_unprocessed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20971"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data=np.array(articles), columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
    "                   mode='cbow',\n",
    "                   min_word_count=5,\n",
    "                   context=5):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "    print('Training Word2Vec model...')\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context)\n",
    "#                                         sample=downsampling)\n",
    "    embedding_model.init_sims(replace=True)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "#     embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "#     for i in range(len(vocabulary_inv)):\n",
    "#         word = vocabulary_inv[i]\n",
    "#         if word in embedding_model:\n",
    "#             embedding_weights[i] = embedding_model[word]\n",
    "#         else:\n",
    "# #             print(\"word not in model: \", word)\n",
    "#             embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "#                                                      embedding_model.vector_size)\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_excluded = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~'\n",
    "\n",
    "def preprocess_df(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    translator = str.maketrans(punc_excluded, ' ' * len(punc_excluded))\n",
    "    preprocessed_sentences = []\n",
    "    for row in df:\n",
    "        sent = row\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().lower().split(' ')\n",
    "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1]\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "#     df[\"text\"] = preprocessed_sentences\n",
    "    return preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = articles[:train_len]\n",
    "df_train = preprocess_df(df_train)\n",
    "df_test = articles[train_len+1:]\n",
    "df_test = preprocess_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model: CBOW\n",
      "Saving Word2Vec model embedding\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train)]\n",
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "inp_data = [[vocabulary[word] for word in text] for text in tagged_data]\n",
    "embedding_model = get_embeddings(inp_data, vocabulary_inv)\n",
    "\n",
    "\n",
    "tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train)]\n",
    "# tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kr20190120984a_',\n",
       " '_the',\n",
       " 'the_self-supporting',\n",
       " 'self-supporting_unmanned',\n",
       " 'unmanned_border',\n",
       " 'border_system',\n",
       " 'system_according',\n",
       " 'according_to',\n",
       " 'to_an',\n",
       " 'an_embodiment',\n",
       " 'embodiment_of',\n",
       " 'of_the',\n",
       " 'the_present',\n",
       " 'present_invention',\n",
       " 'invention_utilizes',\n",
       " 'utilizes_radar',\n",
       " 'radar_and',\n",
       " 'and_the',\n",
       " 'the_internet',\n",
       " 'internet_of',\n",
       " 'of_things',\n",
       " 'things_']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tagged_data.txt','w') as f:\n",
    "#     for item in tagged_data:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_idx_to_word=dict()\n",
    "\n",
    "def get_quality_phrases_embeddings(quality_phrases):\n",
    "    quality_embeddings=[]\n",
    "    for qp in quality_phrases:\n",
    "        qp = qp.lower()\n",
    "        if qp in embedding_model:\n",
    "            embedding_idx_to_word[len(quality_embeddings)] = qp\n",
    "            quality_embeddings.append(embedding_model[qp])\n",
    "        else:\n",
    "            print(qp)\n",
    "            \n",
    "    return quality_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality_phrases_train = get_quality_phrases(lines[:train_len])\n",
    "# quality_phrases_test = get_quality_phrases(lines[train_len+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_phrases_train_raw = [p.replace('_',' ') for p in quality_phrases_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9950"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quality_phrases_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the entire list\n",
    "quality_phrases=set()\n",
    "quality_phrases = quality_phrases.union(quality_phrases_train).union(quality_phrases_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8174"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quality_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_embeddings = get_quality_phrases_embeddings(quality_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8174"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quality_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3149"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quality_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make some predictions? -- using labeled data (directly test the matching phrase.. In pratice, should use n-gram instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert to 2-gram, 3-gram and add to the Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df_text has been preprocessed and ready to be used! -- oh no, it should be unsupervised, so do not need the phrase-to-token step\n",
    "df_test = unprocessed_lines[20001:]\n",
    "df_test = preprocess_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(['black'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.update({\"red\",1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "970it [04:23,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "## Build n-gram vocabulary for df-test\n",
    "## an inverted index might be needed? --- so if satisfy frequency, it will be encoded as a single token\n",
    "## so in the loop, I need to create a n-gram dict(token, idx), a counter, check if it is in the high-quality corpus (quality_phrases_train_raw)\n",
    "# counter_test = Counter()\n",
    "# quality_phrase_val = set()\n",
    "# inverted_index = defaultdict(lambda: list())\n",
    "# for idx, article in tqdm(enumerate(df_test)):\n",
    "#     n_grams = get_ngrams(article, 2)\n",
    "#     temp = defaultdict(lambda:0)\n",
    "#     for gram in n_grams:\n",
    "#         if gram in quality_phrases_train_raw:\n",
    "#             quality_phrase_val.add(gram)\n",
    "#         temp[gram]+=1\n",
    "#         inverted_index[gram].append(idx)\n",
    "\n",
    "#     counter_test.update(temp)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "749it [00:13, 54.53it/s]\n"
     ]
    }
   ],
   "source": [
    "### Senario 2: convert the whole article to bi-grams, save the candidates\n",
    "counter_test = Counter()\n",
    "quality_phrase_val = set()\n",
    "inverted_index = defaultdict(lambda: list())\n",
    "for idx, article in tqdm(enumerate(df_test)):\n",
    "    n_grams = article.split(' ')\n",
    "    temp = defaultdict(lambda:0)\n",
    "    for gram in n_grams:\n",
    "        if gram in quality_phrases_train:\n",
    "            quality_phrase_val.add(gram)\n",
    "        temp[gram]+=1\n",
    "        inverted_index[gram].append(idx)\n",
    "\n",
    "    counter_test.update(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting electricity power lines\n",
    "# Inspecting_electricity electricity_power power_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1186, 47912)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quality_phrase_val), len(counter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now check the counter and prepare candidate set: candidate = satisfy frequency requirment - intersection with quality phrase\n",
    "## data structure for candidates: list()\n",
    "thres = 5\n",
    "candidates = [x for x,count in counter_test.items() if count >= thres and x not in quality_phrase_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4539"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now go back to the test corpus and replace the occurences of candidates with a single phrase token. \n",
    "## Use the inverted index\n",
    "\n",
    "## This is for senario1: replace part of n-gram in the article only\n",
    "# doc_retriver=defaultdict(lambda: set())\n",
    "# for x in candidates:\n",
    "#     for idx in inverted_index[x]:\n",
    "#         doc_retriver[idx].add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huangbeidan is huangbeidan is best'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"beidan is beidan is best\"\n",
    "test.replace(\"beidan\",\"huangbeidan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace in the corpus\n",
    "## attention: can't simply replace! there might be overlaps lmao....\n",
    "\n",
    "# for idx, article in enumerate(df_test):\n",
    "#     if idx not in doc_retriver: continue\n",
    "#     phrase_to_replace = doc_retriver[idx]\n",
    "#     for p in phrase_to_replace:\n",
    "#         df_test[idx] = df_test[idx].replace(p, \"_\".join(p.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Senario 2: compute similarity vectors for the candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update the training model first\n",
    "tagged_train_test = [word_tokenize(_d) for i, _d in enumerate(df_test)]\n",
    "\n",
    "\n",
    "word_counts_c, vocabulary_c, vocabulary_inv_c = build_vocab(tagged_train_test)\n",
    "inp_data_c = [[vocabulary_c[word] for word in text] for text in tagged_train_test]\n",
    "new_sentences = [[vocabulary_inv_c[w] for w in s] for s in inp_data_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "excount=0\n",
    "for line in new_sentences:\n",
    "    excount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325303, 487695)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.build_vocab(new_sentences, update=True)\n",
    "embedding_model.train(new_sentences, total_examples = excount, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate candidates\n",
    "## 1.top similar item should be quality phrase 2.similarity should above certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '_the', 'the_utility', 'utility_model', 'model_proposes']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4539 [00:00<?, ?it/s]/home/beidan/miniconda3/envs/cse291a/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/beidan/miniconda3/envs/cse291a/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "100%|██████████| 4539/4539 [00:11<00:00, 385.63it/s]\n"
     ]
    }
   ],
   "source": [
    "quality_temp=[]\n",
    "for test_cand in tqdm(candidates):\n",
    "    if test_cand not in embedding_model: continue\n",
    "    top_matches = embedding_model.most_similar(positive=[test_cand])[:2]\n",
    "    for match in top_matches:\n",
    "        if match[0] not in quality_phrases_train or match[1] < 0.85: continue\n",
    "        quality_temp.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(657, 1186)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quality_temp), len(quality_phrase_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_temp_words = [p[0] for p in quality_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "surplus = [p for p in quality_temp_words if p not in quality_phrases_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,\n",
       " {'2nd_embodiment',\n",
       "  '3gpp_ts',\n",
       "  'access_nodes',\n",
       "  'active_nn',\n",
       "  'aerial_vehicles',\n",
       "  'air_traffic',\n",
       "  'almanac_data',\n",
       "  'artificial_intelligence',\n",
       "  'autonomous_operation',\n",
       "  'blade_assemblies',\n",
       "  'blanking_mouth',\n",
       "  'block_chain',\n",
       "  'block_diagram',\n",
       "  'boom_spreader',\n",
       "  'bull_stick',\n",
       "  'cage_structure',\n",
       "  'changing_mechanism',\n",
       "  'charging_connector',\n",
       "  'communication_interface',\n",
       "  'compressed_air',\n",
       "  'connecting_pin',\n",
       "  'control_mainboard',\n",
       "  'convex_stupefied',\n",
       "  'coordinate_information',\n",
       "  'cross_correlation',\n",
       "  'depth_information',\n",
       "  'detailed_description',\n",
       "  'discharge_gate',\n",
       "  'docking_mechanism',\n",
       "  'driving_gear',\n",
       "  'driving_unit',\n",
       "  'driving_wheel',\n",
       "  'edge_computing',\n",
       "  'elastic_component',\n",
       "  'electric_supply',\n",
       "  'electricity_generation',\n",
       "  'electrified_rail',\n",
       "  'environmental_monitoring',\n",
       "  'exemplary_embodiments',\n",
       "  'exhaust_fans',\n",
       "  'fixed_link',\n",
       "  'focus_adjusting',\n",
       "  'forced_convection',\n",
       "  'gear_box',\n",
       "  'guide_post',\n",
       "  'handover_destination',\n",
       "  'hijacking_message',\n",
       "  'identification_module',\n",
       "  'idler_wheel',\n",
       "  'ignition_signal',\n",
       "  'inflatable_member',\n",
       "  'injection_signal',\n",
       "  'integrated_circuit',\n",
       "  'intelligent_battery',\n",
       "  'jar_body',\n",
       "  'lash_ship',\n",
       "  'launching_tube',\n",
       "  'limit_plate',\n",
       "  'liquid_hydrogen',\n",
       "  'lock_catch',\n",
       "  'locking_member',\n",
       "  'logistic_car',\n",
       "  'main_casing',\n",
       "  'medicine_box',\n",
       "  'metal_oxide',\n",
       "  'mhd_acceleration',\n",
       "  'mhd_accelerator',\n",
       "  'modified_configuration',\n",
       "  'molten_metal',\n",
       "  'monitor_terminal',\n",
       "  'mounting_rack',\n",
       "  'movable_lens',\n",
       "  'moving_objects',\n",
       "  'moving_target',\n",
       "  'multicopter_90b',\n",
       "  'ocean_current',\n",
       "  'operation_plot',\n",
       "  'optical_parallax',\n",
       "  'optimal_path',\n",
       "  'perform_tasks',\n",
       "  'photographic_head',\n",
       "  'picture_biography',\n",
       "  'previous_points',\n",
       "  'protection_casing',\n",
       "  'pseudo-gps_signal',\n",
       "  'pushing_component',\n",
       "  'quadrotor_drone',\n",
       "  'ratchet_bar',\n",
       "  'recognition_rate',\n",
       "  'relative_distance',\n",
       "  'ring_stand',\n",
       "  'rotor_arms',\n",
       "  'runing_rest',\n",
       "  'safe_mode',\n",
       "  'screw_mount',\n",
       "  'sea_surface',\n",
       "  'server_receives',\n",
       "  'shock_tunnel',\n",
       "  'sliding_slot',\n",
       "  'small_drone',\n",
       "  'snap_ring',\n",
       "  'solid_fixed',\n",
       "  'specific_embodiment',\n",
       "  'spindle_motor',\n",
       "  'sprinkler_body',\n",
       "  'steering_spindle',\n",
       "  'storage_bin',\n",
       "  'subsequent_state',\n",
       "  'supply_unit',\n",
       "  'tie_rod',\n",
       "  'traffic_management',\n",
       "  'translational_acceleration',\n",
       "  'unmanned_vehicle',\n",
       "  'vertical_fin',\n",
       "  'video_acquisition',\n",
       "  'voice_coil',\n",
       "  'vr_models',\n",
       "  'wing_flap',\n",
       "  'wireless_access',\n",
       "  'wireless_lan'})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(surplus)), set(surplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "surplus_write = [p.replace('_',' ') for p in surplus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Active Learning Injection Part - add the surplus phrases to Active Learning expert label file\n",
    "with open('input/patentdb-medium-expert.txt', 'w') as f:\n",
    "    f.writelines(\"%s\\n\" % p for p in surplus_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):\n",
    "    return cosine_similarity(feature_vec_1.reshape(1,-1), feature_vec_2.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3524833"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cosine_similarity(quality_embeddings[0],quality_embeddings[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beidan/miniconda3/envs/cse291a/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gear_shaft', 0.9258540868759155),\n",
       " ('driven_shaft', 0.9127465486526489),\n",
       " ('with_rotary', 0.9122529029846191),\n",
       " ('circular_shaft', 0.9077884554862976),\n",
       " ('leading_screw', 0.9030115604400635),\n",
       " ('on_agitating', 0.9028359651565552),\n",
       " ('articulated_shaft', 0.9022598266601562),\n",
       " ('hollow_shaft', 0.8986348509788513),\n",
       " ('shaft_side', 0.8972604870796204),\n",
       " ('roll_shaft', 0.8969671130180359)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.most_similar(positive=['agitating_shaft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embodiments\n",
      "present_invention\n",
      "disclose\n",
      "kind\n",
      "power_supply\n",
      "solar_energy\n",
      "unmanned\n",
      "plane\n",
      "power_supply\n",
      "includes\n",
      "power_supply\n",
      "module\n",
      "current_voltage\n",
      "monitoring_modular\n",
      "first\n",
      "voltage_conversion_module\n",
      "second\n",
      "voltage_conversion_module\n",
      "tertiary\n",
      "voltage_conversion_module\n",
      "voltage\n",
      "power_supply\n",
      "module\n",
      "subjected\n",
      "dc\n",
      "dc\n",
      "first\n",
      "voltage_conversion_module\n",
      "second\n",
      "voltage_conversion_module\n",
      "tertiary\n",
      "voltage_conversion_module\n",
      "converted\n",
      "three\n",
      "kinds\n",
      "electric\n",
      "signals\n",
      "meets\n",
      "requirement\n",
      "unmanned_electromechanical_source\n",
      "multi\n",
      "output\n",
      "hand\n",
      "electric_current\n",
      "exported\n",
      "power_supply\n",
      "module\n",
      "current_voltage\n",
      "monitoring_modular\n",
      "one\n",
      "hand\n",
      "electric_current\n",
      "input\n",
      "first\n",
      "voltage_conversion_module\n",
      "second\n",
      "voltage_conversion_module\n",
      "carries\n",
      "conversion\n",
      "voltage\n",
      "hand\n",
      "electric_current\n",
      "power_supply\n",
      "module\n",
      "output\n",
      "monitored\n",
      "ensure\n",
      "circuit\n",
      "safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beidan/miniconda3/envs/cse291a/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_article = tagged_test_data[3]\n",
    "for token in test_article:\n",
    "    if token in embedding_model:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "##online word2vec???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('test_c.txt', 'w') as f:\n",
    "#     f.write(articles[20001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_article.append('information_security')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_c, vocabulary_c, vocabulary_inv_c = build_vocab([test_article])\n",
    "inp_data_c = [[vocabulary_c[word] for word in text] for text in [test_article]]\n",
    "new_sentences = [[vocabulary_inv_c[w] for w in s] for s in inp_data_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "excount=0\n",
    "for line in new_sentences:\n",
    "    excount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.build_vocab(new_sentences, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 213)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.train(new_sentences, total_examples = excount, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beidan/miniconda3/envs/cse291a/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01287237,  0.0090122 , -0.00884458,  0.00848831,  0.00032224,\n",
       "       -0.00666879, -0.0013857 , -0.00014764, -0.00687364,  0.00806865,\n",
       "       -0.00601127, -0.00032863, -0.00272678, -0.01601487, -0.00389749,\n",
       "        0.00042824,  0.00086173,  0.00985634,  0.00572525, -0.00297683,\n",
       "       -0.01500027, -0.00444778, -0.00254812, -0.01314077,  0.01753565,\n",
       "       -0.00735723,  0.00334592,  0.01917579,  0.00578594, -0.00970757,\n",
       "        0.00142931, -0.01196378,  0.00858092, -0.00496101,  0.00096193,\n",
       "       -0.00423299,  0.0003428 ,  0.00543864,  0.0143293 , -0.00043601,\n",
       "        0.00637902,  0.00132579, -0.00044674, -0.0150325 ,  0.00677556,\n",
       "        0.00469601, -0.00556763,  0.00105794,  0.00155691,  0.0041729 ,\n",
       "       -0.00970957, -0.00418425,  0.00195676, -0.01023265, -0.00713241,\n",
       "        0.0017502 ,  0.0062706 , -0.00058419,  0.00763606,  0.00431534,\n",
       "       -0.00032861,  0.01195908, -0.00846418,  0.00086708, -0.00189516,\n",
       "        0.00062416, -0.00613898, -0.01201752,  0.00125749,  0.0007598 ,\n",
       "       -0.00542694, -0.00195966,  0.0042773 ,  0.00187989,  0.00548098,\n",
       "       -0.0017548 ,  0.00838181,  0.00808381, -0.01256359, -0.00705223,\n",
       "       -0.00459955, -0.00519263, -0.00124315, -0.00403855, -0.00542657,\n",
       "       -0.01764286,  0.0062472 ,  0.00863983,  0.00511664,  0.0040605 ,\n",
       "       -0.01740051,  0.00194638, -0.01036619,  0.00365548,  0.00289508,\n",
       "       -0.00938043, -0.01857208,  0.00222451,  0.0058123 ,  0.0175708 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model['voltage_conversion_module']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alenv",
   "language": "python",
   "name": "alenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
